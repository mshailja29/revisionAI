{
    "title": "Sample PDF Document",
    "summary": "In Week 12 of FE524, the focus was on using local Large Language Models (LLMs) by downloading model weights, setting up the necessary tools, and running prompts. Key steps included initializing Git LFS for downloading gated models from Hugging Face, cloning the llama.cpp repository, and building it with CMake. Users were instructed to convert model files from .safetensors to .gguf using a provided script. The llama-cli utility allows users to run prompts directly, while llama-server sets up a local API compatible with OpenAI's completions API. Additionally, Python's subprocess package was discussed for managing command-line calls from scripts. For Homework 12, students are tasked with installing llama.cpp, downloading and converting model weights, launching a local server, and modifying existing code to interact with this server, culminating in the submission of their scripts and outputs.",
    "flashcards": [
        {
            "Question": "What is the first step to download model weights for LLMs?",
            "Answer": "Set up Git LFS with the command 'git lfs install'."
        },
        {
            "Question": "How do you convert model weights from .safetensors to .gguf?",
            "Answer": "Run the commands 'python -m pip install \u2013r requirements.txt' and 'python convert_hf_to_gguf.py /path/to/model/directory'."
        },
        {
            "Question": "What command is used to run llama-server on port 8080?",
            "Answer": "'./build/bin/llama-server -m models/Llama-3.2-1B-Instruct/Llama-3.2-1B-Instruct-F16.gguf --port 8080'."
        },
        {
            "Question": "What is the purpose of the subprocess package in Python?",
            "Answer": "It allows shell commands or binaries to be called from Python, capturing output and handling errors."
        },
        {
            "Question": "What is the primary change needed to call llama-server from previous GPT API code?",
            "Answer": "Change the root URL from 'https://api.openai.com' to 'http://localhost:8080'."
        }
    ],
    "quizzes": [
        {
            "question": "What must be set up before downloading model weights using git?",
            "options": [
                "Git LFS",
                "Python",
                "Node.js",
                "CMake"
            ],
            "answer": "Git LFS"
        },
        {
            "question": "What command is used to install the dependencies listed in requirements.txt?",
            "options": [
                "python -m pip install",
                "git clone",
                "cmake --build",
                "pip install requirements"
            ],
            "answer": "python -m pip install"
        },
        {
            "question": "Which command is used to run the llama-server?",
            "options": [
                "./build/bin/llama-server -m models/Llama-3.2-1B-Instruct/Llama-3.2-1B-Instruct-F16.gguf --port 8080",
                "./build/bin/llama-cli",
                "git clone <url>",
                "cmake -B build"
            ],
            "answer": "./build/bin/llama-server -m models/Llama-3.2-1B-Instruct/Llama-3.2-1B-Instruct-F16.gguf --port 8080"
        },
        {
            "question": "What type of model file do you need to convert the .safetensors to?",
            "options": [
                ".gguf",
                ".json",
                ".csv",
                ".txt"
            ],
            "answer": ".gguf"
        },
        {
            "question": "What is suggested for running Python scripts that call the llama-server?",
            "options": [
                "Use requests instead of openai package",
                "Use the FTP protocol",
                "Use an email attachment",
                "Use Python 2"
            ],
            "answer": "Use requests instead of openai package"
        }
    ]
}